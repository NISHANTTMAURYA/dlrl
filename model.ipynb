{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2953e2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: LOADING DATA\n",
      "============================================================\n",
      "\n",
      "Training windows: (410623, 60, 5)\n",
      "  - Number of windows: 410,623\n",
      "  - Timesteps per window: 60\n",
      "  - Features per timestep: 5\n",
      "\n",
      "Test windows: (105936, 60, 5)\n",
      "  - Number of windows: 105,936\n",
      "  - Timesteps per window: 60\n",
      "  - Features per timestep: 5\n",
      "\n",
      "✓ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 1: LOADING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the windows we created\n",
    "X_train = np.load('dataset/X_train.npy')\n",
    "X_test = np.load('dataset/X_test.npy')\n",
    "\n",
    "print(f\"\\nTraining windows: {X_train.shape}\")\n",
    "print(f\"  - Number of windows: {X_train.shape[0]:,}\")\n",
    "print(f\"  - Timesteps per window: {X_train.shape[1]}\")\n",
    "print(f\"  - Features per timestep: {X_train.shape[2]}\")\n",
    "\n",
    "print(f\"\\nTest windows: {X_test.shape}\")\n",
    "print(f\"  - Number of windows: {X_test.shape[0]:,}\")\n",
    "print(f\"  - Timesteps per window: {X_test.shape[1]}\")\n",
    "print(f\"  - Features per timestep: {X_test.shape[2]}\")\n",
    "\n",
    "print(\"\\n✓ Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92502132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: BUILDING LSTM AUTOENCODER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model parameters\n",
    "TIMESTEPS = 60  # 60 seconds\n",
    "FEATURES = 5    # HR, HR_change, HR_rolling_mean, HR_rolling_std, HR_deviation\n",
    "\n",
    "def build_autoencoder():\n",
    "    \"\"\"\n",
    "    Build LSTM Autoencoder\n",
    "    \n",
    "    Encoder: Compresses 60 seconds of data into a small summary\n",
    "    Decoder: Tries to recreate the original 60 seconds from summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # INPUT: 60 seconds × 5 features\n",
    "    input_layer = layers.Input(shape=(TIMESTEPS, FEATURES))\n",
    "    \n",
    "    # ============ ENCODER (Compressor) ============\n",
    "    # LSTM Layer 1: Learn patterns\n",
    "    encoded = layers.LSTM(64, activation='relu', return_sequences=True)(input_layer)\n",
    "    encoded = layers.Dropout(0.2)(encoded)  # Prevents overfitting\n",
    "    \n",
    "    # LSTM Layer 2: Compress more\n",
    "    encoded = layers.LSTM(32, activation='relu', return_sequences=False)(encoded)\n",
    "    encoded = layers.Dropout(0.2)(encoded)\n",
    "    \n",
    "    # BOTTLENECK: Tiny summary (16 numbers represent entire 60-second pattern!)\n",
    "    bottleneck = layers.Dense(16, activation='relu')(encoded)\n",
    "    \n",
    "    # ============ DECODER (Recreator) ============\n",
    "    # Expand the summary back to 60 timesteps\n",
    "    decoded = layers.RepeatVector(TIMESTEPS)(bottleneck)\n",
    "    \n",
    "    # LSTM Layer 3: Start recreating\n",
    "    decoded = layers.LSTM(32, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = layers.Dropout(0.2)(decoded)\n",
    "    \n",
    "    # LSTM Layer 4: Continue recreating\n",
    "    decoded = layers.LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = layers.Dropout(0.2)(decoded)\n",
    "    \n",
    "    # OUTPUT: Recreated 60 seconds × 5 features\n",
    "    output_layer = layers.TimeDistributed(layers.Dense(FEATURES))(decoded)\n",
    "    \n",
    "    # Build the model\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Create the model\n",
    "model = build_autoencoder()\n",
    "\n",
    "# Compile (prepare for training)\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer (smart learning algorithm)\n",
    "    loss='mse'         # MSE = Mean Squared Error (measures reconstruction error)\n",
    ")\n",
    "\n",
    "# Show model structure\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n✓ Model built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 3: TRAINING THE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split training data into train and validation (90-10)\n",
    "split_index = int(0.9 * len(X_train))\n",
    "X_train_split = X_train[:split_index]\n",
    "X_val_split = X_train[split_index:]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train_split):,}\")\n",
    "print(f\"Validation samples: {len(X_val_split):,}\")\n",
    "\n",
    "# Callbacks (smart training helpers)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Watch validation loss\n",
    "    patience=5,              # Stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True  # Keep the best model\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',      # Save best model\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"This may take 10-30 minutes depending on your computer!\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train the model\n",
    "# Input = X_train, Output = X_train (trying to recreate itself!)\n",
    "history = model.fit(\n",
    "    X_train_split, X_train_split,  # Input and output are same!\n",
    "    epochs=50,                      # Try 50 rounds\n",
    "    batch_size=32,                  # Process 32 windows at a time\n",
    "    validation_data=(X_val_split, X_val_split),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1                       # Show progress\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"✓ Best model saved as 'best_model.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PART 4: VISUALIZING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Model Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Model Training Progress (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Training history plotted!\")\n",
    "print(\"✓ Saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab95982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PART 5: CALCULATING RECONSTRUCTION ERRORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best model\n",
    "model = keras.models.load_model('best_model.keras')\n",
    "\n",
    "print(\"\\nCalculating errors on TRAINING data...\")\n",
    "# Predict (recreate) training data\n",
    "train_predictions = model.predict(X_train, verbose=1)\n",
    "\n",
    "# Calculate error for each window\n",
    "# MSE = Mean Squared Error = average of (actual - predicted)^2\n",
    "train_errors = np.mean(np.square(X_train - train_predictions), axis=(1, 2))\n",
    "\n",
    "print(f\"\\nTraining error statistics:\")\n",
    "print(f\"  Mean error: {train_errors.mean():.4f}\")\n",
    "print(f\"  Std error: {train_errors.std():.4f}\")\n",
    "print(f\"  Min error: {train_errors.min():.4f}\")\n",
    "print(f\"  Max error: {train_errors.max():.4f}\")\n",
    "\n",
    "print(\"\\nCalculating errors on TEST data...\")\n",
    "# Predict (recreate) test data\n",
    "test_predictions = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Calculate error for each window\n",
    "test_errors = np.mean(np.square(X_test - test_predictions), axis=(1, 2))\n",
    "\n",
    "print(f\"\\nTest error statistics:\")\n",
    "print(f\"  Mean error: {test_errors.mean():.4f}\")\n",
    "print(f\"  Std error: {test_errors.std():.4f}\")\n",
    "print(f\"  Min error: {test_errors.min():.4f}\")\n",
    "print(f\"  Max error: {test_errors.max():.4f}\")\n",
    "\n",
    "print(\"\\n✓ Errors calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PART 6: SETTING THRESHOLD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set threshold at 95th percentile of training errors\n",
    "# Meaning: 95% of normal patterns have error below this\n",
    "threshold = np.percentile(train_errors, 95)\n",
    "\n",
    "print(f\"\\nThreshold (95th percentile): {threshold:.4f}\")\n",
    "print(f\"Meaning: Any error above {threshold:.4f} is considered unusual\")\n",
    "\n",
    "# Count anomalies in test set\n",
    "test_anomalies = test_errors > threshold\n",
    "n_anomalies = test_anomalies.sum()\n",
    "\n",
    "print(f\"\\nTest set results:\")\n",
    "print(f\"  Total windows: {len(test_errors):,}\")\n",
    "print(f\"  Anomalies detected: {n_anomalies:,}\")\n",
    "print(f\"  Anomaly rate: {(n_anomalies/len(test_errors))*100:.2f}%\")\n",
    "\n",
    "# Visualize errors\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_errors, bins=50, alpha=0.7, label='Training', color='blue')\n",
    "plt.hist(test_errors, bins=50, alpha=0.7, label='Test', color='green')\n",
    "plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_errors, alpha=0.7, label='Test Error', linewidth=0.5)\n",
    "plt.axhline(threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "plt.scatter(np.where(test_anomalies)[0], test_errors[test_anomalies], \n",
    "           color='red', label='Anomalies', s=10, alpha=0.6)\n",
    "plt.xlabel('Window Index')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Test Set Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('anomaly_detection_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Threshold set!\")\n",
    "print(\"✓ Results saved as 'anomaly_detection_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc21e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PART 7: SAVING FOR DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save threshold\n",
    "np.save('threshold.npy', threshold)\n",
    "print(f\"✓ Saved threshold: {threshold:.4f}\")\n",
    "\n",
    "# Model already saved as 'best_model.keras'\n",
    "print(f\"✓ Model saved as: best_model.keras\")\n",
    "\n",
    "# Scaler already saved from earlier\n",
    "print(f\"✓ Scaler saved as: scaler.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL DONE! MODEL IS READY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFiles you have:\")\n",
    "print(\"  1. best_model.keras - The trained LSTM model\")\n",
    "print(\"  2. scaler.pkl - For normalizing new data\")\n",
    "print(\"  3. threshold.npy - The anomaly detection threshold\")\n",
    "\n",
    "print(\"\\nYou can now use these in a smartwatch app!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
