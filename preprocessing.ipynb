{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ad8d70",
   "metadata": {},
   "source": [
    "# Treadmill Maximal Exercise Tests Dataset\n",
    "###### [Link](https://physionet.org/content/treadmill-exercise-cardioresp/1.0.1/)\n",
    "\n",
    "This dataset contains cardiorespiratory measurements taken during 992 treadmill maximal graded exercise tests conducted at the Exercise Physiology and Human Performance Lab, University of Malaga.\n",
    "\n",
    "## File: `test_measure.csv`\n",
    "\n",
    "This file contains all breath-by-breath cardiorespiratory measurements for each graded effort test.\n",
    "\n",
    "### General Info\n",
    "\n",
    "- **Rows:** 575,087 (one per breath measurement)\n",
    "- **Tests:** 992\n",
    "- **Median measurements per test:** 580 [IQR: 484–673]\n",
    "- **Median test duration:** 1,093.00 seconds [IQR: 978.75–1,208.00]\n",
    "\n",
    "### Variables\n",
    "\n",
    "| Name     | Description                                | Unit                  |\n",
    "|----------|--------------------------------------------|-----------------------|\n",
    "| time     | Time since measurement started             | seconds               |\n",
    "| Speed    | Treadmill speed                            | km/h                  |\n",
    "| HR       | Heart rate                                 | beats per minute      |\n",
    "| VO2      | Oxygen consumption                         | mL/min                |\n",
    "| VCO2     | Carbon dioxide production                  | mL/min                |\n",
    "| RR       | Respiration rate                           | respirations/min      |\n",
    "| VE       | Pulmonary ventilation                      | L/min                 |\n",
    "| ID       | Participant identification                 | -                     |\n",
    "| ID_test  | Effort test identification                 | -                     |\n",
    "\n",
    "_Note: VO2, VCO2, and VE are missing for 30 tests._\n",
    "\n",
    "**ID_test** is formatted as `{participant_id}_{test_number}`, e.g., `245_3` = third test of participant 245.\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:**  \n",
    "Mongin, D., García Romero, J., & Alvero Cruz, J. R. (2021). Treadmill Maximal Exercise Tests from the Exercise Physiology and Human Performance Lab of the University of Malaga (version 1.0.1). PhysioNet. https://doi.org/10.13026/7ezk-j442\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9c72c-d382-45ff-b0b5-aaab4d63a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: REMOVING UNNECESSARY COLUMNS\n",
      "============================================================\n",
      "\n",
      "Original columns: ['time', 'Speed', 'HR', 'VO2', 'VCO2', 'RR', 'VE', 'ID_test', 'ID']\n",
      "Total rows: 575,087\n",
      "\n",
      "Columns after cleaning: ['time', 'Speed', 'HR', 'ID_test', 'ID']\n",
      "Removed columns: VO2, VCO2, RR, VE\n",
      "\n",
      "First 10 rows of cleaned data:\n",
      "   time  Speed    HR ID_test  ID\n",
      "0     0    5.0  63.0     2_1   2\n",
      "1     2    5.0  75.0     2_1   2\n",
      "2     4    5.0  82.0     2_1   2\n",
      "3     7    5.0  87.0     2_1   2\n",
      "4     9    5.0  92.0     2_1   2\n",
      "5    11    5.0  94.0     2_1   2\n",
      "6    14    5.0  95.0     2_1   2\n",
      "7    16    5.0  96.0     2_1   2\n",
      "8    17    5.0  97.0     2_1   2\n",
      "9    19    5.0  97.0     2_1   2\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575087 entries, 0 to 575086\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   time     575087 non-null  int64  \n",
      " 1   Speed    575087 non-null  float64\n",
      " 2   HR       574106 non-null  float64\n",
      " 3   ID_test  575087 non-null  object \n",
      " 4   ID       575087 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 21.9+ MB\n",
      "None\n",
      "\n",
      "============================================================\n",
      "✓ STEP 1 COMPLETE!\n",
      "✓ Saved as: output_step1.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#  clean the uncessary columns in the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('test_measure.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: REMOVING UNNECESSARY COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show what we have\n",
    "print(f\"\\nOriginal columns: {df.columns.tolist()}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "\n",
    "# Keep only the columns we need\n",
    "columns_to_keep = ['time', 'Speed', 'HR', 'ID_test', 'ID']\n",
    "\n",
    "df_cleaned = df[columns_to_keep]\n",
    "\n",
    "# Show what we kept\n",
    "print(f\"\\nColumns after cleaning: {df_cleaned.columns.tolist()}\")\n",
    "print(f\"Removed columns: VO2, VCO2, RR, VE\")\n",
    "\n",
    "# Check the data\n",
    "print(\"\\nFirst 10 rows of cleaned data:\")\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\"\\nData info:\")\n",
    "print(df_cleaned.info())\n",
    "\n",
    "# Save to new CSV\n",
    "df_cleaned.to_csv('output_step1.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ STEP 1 COMPLETE!\")\n",
    "print(\"✓ Saved as: output_step1.csv\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total participants: 857\n",
      "Training participants: 685\n",
      "Testing participants: 172\n",
      "\n",
      "Training rows: 457,351\n",
      "Testing rows: 117,736\n",
      "\n",
      "✓ Saved train_data.csv\n",
      "✓ Saved test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# seperate testing and training data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data from step 1\n",
    "df = pd.read_csv('output_step1.csv')\n",
    "\n",
    "# Get unique participant IDs\n",
    "unique_participants = df['ID'].unique()\n",
    "print(f\"Total participants: {len(unique_participants)}\")\n",
    "\n",
    "# Split participants 80-20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    unique_participants, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training participants: {len(train_ids)}\")\n",
    "print(f\"Testing participants: {len(test_ids)}\")\n",
    "\n",
    "# Split the data based on participant ID\n",
    "train_df = df[df['ID'].isin(train_ids)]\n",
    "test_df = df[df['ID'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nTraining rows: {len(train_df):,}\")\n",
    "print(f\"Testing rows: {len(test_df):,}\")\n",
    "\n",
    "# Save both files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Saved train_data.csv\")\n",
    "print(\"✓ Saved test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44513247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: CREATING FEATURES\n",
      "============================================================\n",
      "\n",
      "Original columns: ['time', 'Speed', 'HR', 'ID_test', 'ID']\n",
      "New columns added: ['HR_change', 'Speed_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "Sample data with new features:\n",
      "    time     HR  HR_change  HR_rolling_mean  HR_deviation\n",
      "0      0    0.0        0.0         0.000000      0.000000\n",
      "1      2    0.0        0.0         0.000000      0.000000\n",
      "2      5   54.0        0.0        54.000000      0.000000\n",
      "3      7    0.0        0.0        54.000000      0.000000\n",
      "4      9   91.0        0.0        72.500000     18.500000\n",
      "5     12   93.0        2.0        79.333333     13.666667\n",
      "6     14   94.0        1.0        83.000000     11.000000\n",
      "7     16   95.0        1.0        85.400000      9.600000\n",
      "8     18   94.0       -1.0        86.833333      7.166667\n",
      "9     21   93.0       -1.0        87.714286      5.285714\n",
      "10    26   93.0        0.0        88.375000      4.625000\n",
      "11    27   93.0        0.0        88.888889      4.111111\n",
      "12    32   94.0        1.0        89.400000      4.600000\n",
      "13    34   95.0        1.0        89.909091      5.090909\n",
      "14    35   94.0       -1.0        90.250000      3.750000\n",
      "15    37   93.0       -1.0        90.461538      2.538462\n",
      "16    39   93.0        0.0        90.642857      2.357143\n",
      "17    41   93.0        0.0        90.800000      2.200000\n",
      "18    43   92.0       -1.0        90.875000      1.125000\n",
      "19    45   91.0       -1.0        90.882353      0.117647\n",
      "20    48   90.0       -1.0        90.833333     -0.833333\n",
      "21    50   90.0        0.0        90.789474     -0.789474\n",
      "22    52   90.0        0.0        90.750000     -0.750000\n",
      "23    54   90.0        0.0        90.714286     -0.714286\n",
      "24    56   90.0        0.0        90.681818     -0.681818\n",
      "25    59   92.0        2.0        90.739130      1.260870\n",
      "26    61   95.0        3.0        90.916667      4.083333\n",
      "27    64   97.0        2.0        91.160000      5.840000\n",
      "28    66   97.0        0.0        91.384615      5.615385\n",
      "29    67   99.0        2.0        91.666667      7.333333\n",
      "30    69  100.0        1.0        91.964286      8.035714\n",
      "31    71   99.0       -1.0        92.206897      6.793103\n",
      "32    74   99.0        0.0        93.758621      5.241379\n",
      "33    76   99.0        0.0        93.933333      5.066667\n",
      "34    78   97.0       -2.0        94.133333      2.866667\n",
      "\n",
      "✓ Saved train_data_with_features.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c5a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: CREATING FEATURES FOR BOTH TRAIN AND TEST\n",
      "============================================================\n",
      "\n",
      "Processing TRAIN data...\n",
      "✓ Train rows: 457,351\n",
      "✓ Saved train_data_with_features.csv\n",
      "\n",
      "Processing TEST data...\n",
      "✓ Test rows: 117,736\n",
      "✓ Saved test_data_with_features.csv\n",
      "\n",
      "New columns added:\n",
      "['HR_change', 'Speed_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "Sample from train data:\n",
      "   time    HR  HR_change  HR_rolling_mean  HR_deviation\n",
      "0     0   0.0        0.0         0.000000      0.000000\n",
      "1     2   0.0        0.0         0.000000      0.000000\n",
      "2     5  54.0        0.0        54.000000      0.000000\n",
      "3     7   0.0        0.0        54.000000      0.000000\n",
      "4     9  91.0        0.0        72.500000     18.500000\n",
      "5    12  93.0        2.0        79.333333     13.666667\n",
      "6    14  94.0        1.0        83.000000     11.000000\n",
      "7    16  95.0        1.0        85.400000      9.600000\n",
      "8    18  94.0       -1.0        86.833333      7.166667\n",
      "9    21  93.0       -1.0        87.714286      5.285714\n"
     ]
    }
   ],
   "source": [
    "# create new features \n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: CREATING FEATURES FOR BOTH TRAIN AND TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function to add features\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by ID_test and time to ensure correct order\n",
    "    df = df.sort_values(['ID_test', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Process each test separately\n",
    "    for test_id in df['ID_test'].unique():\n",
    "        mask = df['ID_test'] == test_id\n",
    "        \n",
    "        # 1. HR_change: How much HR changed from previous second\n",
    "        df.loc[mask, 'HR_change'] = df.loc[mask, 'HR'].diff()\n",
    "        \n",
    "        # 2. Speed_change: How much speed changed\n",
    "        df.loc[mask, 'Speed_change'] = df.loc[mask, 'Speed'].diff()\n",
    "        \n",
    "        # 3. HR_rolling_mean: Average HR over last 30 seconds\n",
    "        df.loc[mask, 'HR_rolling_mean'] = df.loc[mask, 'HR'].rolling(window=30, min_periods=1).mean()\n",
    "        \n",
    "        # 4. HR_rolling_std: Variability in HR over last 30 seconds\n",
    "        df.loc[mask, 'HR_rolling_std'] = df.loc[mask, 'HR'].rolling(window=30, min_periods=1).std()\n",
    "        \n",
    "        # 5. HR_deviation: How far is current HR from recent average\n",
    "        df.loc[mask, 'HR_deviation'] = df.loc[mask, 'HR'] - df.loc[mask, 'HR_rolling_mean']\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process training data\n",
    "print(\"\\nProcessing TRAIN data...\")\n",
    "train_df = pd.read_csv('dataset/train_data.csv')\n",
    "train_with_features = add_features(train_df)\n",
    "train_with_features.to_csv('train_data_with_features.csv', index=False)\n",
    "print(f\"✓ Train rows: {len(train_with_features):,}\")\n",
    "print(\"✓ Saved train_data_with_features.csv\")\n",
    "\n",
    "# Process test data\n",
    "print(\"\\nProcessing TEST data...\")\n",
    "test_df = pd.read_csv('dataset/test_data.csv')\n",
    "test_with_features = add_features(test_df)\n",
    "test_with_features.to_csv('test_data_with_features.csv', index=False)\n",
    "print(f\"✓ Test rows: {len(test_with_features):,}\")\n",
    "print(\"✓ Saved test_data_with_features.csv\")\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "print(['HR_change', 'Speed_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation'])\n",
    "\n",
    "print(\"\\nSample from train data:\")\n",
    "print(train_with_features[['time', 'HR', 'HR_change', 'HR_rolling_mean', 'HR_deviation']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec4210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: NORMALIZING DATA\n",
      "============================================================\n",
      "\n",
      "Train rows: 457,351\n",
      "Test rows: 117,736\n",
      "\n",
      "Normalizing these columns: ['HR', 'Speed', 'HR_change', 'Speed_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "Scaler fitted on training data\n",
      "Mean values learned: [1.46849982e+02 9.61521086e+00 4.78254120e-02 1.49884881e-03\n",
      " 1.46103142e+02 4.05485994e+00 8.23860652e-01]\n",
      "Std values learned: [32.6890873   4.52100952  1.78088176  0.50075904 33.07701309  3.55716185\n",
      "  6.95052833]\n",
      "\n",
      "✓ Both datasets normalized\n",
      "\n",
      "Sample normalized values (train data):\n",
      "         HR     Speed  HR_change  HR_deviation\n",
      "0 -4.492324 -1.020836  -0.026855     -0.118532\n",
      "1 -4.492324 -1.020836  -0.026855     -0.118532\n",
      "2 -2.840397 -1.020836  -0.026855     -0.118532\n",
      "3 -4.492324 -1.020836  -0.026855     -0.118532\n",
      "4 -1.708521 -1.020836  -0.026855      2.543136\n",
      "5 -1.647338 -1.020836   1.096184      1.847745\n",
      "6 -1.616747 -1.020836   0.534665      1.464081\n",
      "7 -1.586156 -1.020836   0.534665      1.262658\n",
      "8 -1.616747 -1.020836  -0.588374      0.912565\n",
      "9 -1.647338 -1.020836  -0.588374      0.641945\n",
      "\n",
      "✓ Saved train_data_normalized.csv\n",
      "✓ Saved test_data_normalized.csv\n",
      "✓ Saved scaler.pkl (you'll need this for real-time predictions!)\n",
      "\n",
      "============================================================\n",
      "NORMALIZATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: NORMALIZING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data with features\n",
    "train_df = pd.read_csv('dataset/train_data_with_features.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_with_features.csv')\n",
    "\n",
    "print(f\"\\nTrain rows: {len(train_df):,}\")\n",
    "print(f\"Test rows: {len(test_df):,}\")\n",
    "\n",
    "# Features we want to normalize\n",
    "features_to_normalize = [\n",
    "    'HR', \n",
    "    'Speed', \n",
    "    'HR_change', \n",
    "    'Speed_change', \n",
    "    'HR_rolling_mean', \n",
    "    'HR_rolling_std', \n",
    "    'HR_deviation'\n",
    "]\n",
    "\n",
    "print(f\"\\nNormalizing these columns: {features_to_normalize}\")\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT the scaler ONLY on training data\n",
    "scaler.fit(train_df[features_to_normalize])\n",
    "\n",
    "print(\"\\nScaler fitted on training data\")\n",
    "print(\"Mean values learned:\", scaler.mean_)\n",
    "print(\"Std values learned:\", scaler.scale_)\n",
    "\n",
    "# TRANSFORM both train and test using the same scaler\n",
    "train_df[features_to_normalize] = scaler.transform(train_df[features_to_normalize])\n",
    "test_df[features_to_normalize] = scaler.transform(test_df[features_to_normalize])\n",
    "\n",
    "print(\"\\n✓ Both datasets normalized\")\n",
    "\n",
    "# Show before/after example\n",
    "print(\"\\nSample normalized values (train data):\")\n",
    "print(train_df[['HR', 'Speed', 'HR_change', 'HR_deviation']].head(10))\n",
    "\n",
    "# Save normalized data\n",
    "train_df.to_csv('dataset/train_data_normalized.csv', index=False)\n",
    "test_df.to_csv('dataset/test_data_normalized.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Saved train_data_normalized.csv\")\n",
    "print(\"✓ Saved test_data_normalized.csv\")\n",
    "\n",
    "# Save the scaler (IMPORTANT for later!)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✓ Saved scaler.pkl (you'll need this for real-time predictions!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NORMALIZATION COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0cfd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REMOVING SPEED COLUMNS\n",
      "============================================================\n",
      "\n",
      "Original columns: ['time', 'Speed', 'HR', 'ID_test', 'ID', 'HR_change', 'Speed_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "Removed: ['Speed', 'Speed_change']\n",
      "Remaining columns: ['time', 'HR', 'ID_test', 'ID', 'HR_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "✓ Updated train_data_normalized.csv\n",
      "✓ Updated test_data_normalized.csv\n",
      "\n",
      "Features now available for model:\n",
      "['HR', 'HR_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "Total: 5 features\n"
     ]
    }
   ],
   "source": [
    "#  we should remove the speed\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REMOVING SPEED COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the normalized data\n",
    "train_df = pd.read_csv('dataset/train_data_normalized.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_normalized.csv')\n",
    "\n",
    "print(f\"\\nOriginal columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Remove Speed and Speed_change columns\n",
    "columns_to_remove = ['Speed', 'Speed_change']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_remove)\n",
    "test_df = test_df.drop(columns=columns_to_remove)\n",
    "\n",
    "print(f\"\\nRemoved: {columns_to_remove}\")\n",
    "print(f\"Remaining columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Save back to same files\n",
    "train_df.to_csv('dataset/train_data_normalized.csv', index=False)\n",
    "test_df.to_csv('dataset/test_data_normalized.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Updated train_data_normalized.csv\")\n",
    "print(f\"✓ Updated test_data_normalized.csv\")\n",
    "\n",
    "print(\"\\nFeatures now available for model:\")\n",
    "feature_columns = ['HR', 'HR_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
    "print(feature_columns)\n",
    "print(f\"Total: {len(feature_columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7056f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: CREATING 60-SECOND WINDOWS (HR ONLY)\n",
      "============================================================\n",
      "\n",
      "Train rows: 457,351\n",
      "Test rows: 117,736\n",
      "\n",
      "Using 5 features per timestep\n",
      "Features: ['HR', 'HR_change', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation']\n",
      "\n",
      "Creating windows for TRAIN data...\n",
      "✓ Created 410,623 training windows\n",
      "  Each window shape: (60, 5) (60 timesteps × 5 features)\n",
      "\n",
      "Creating windows for TEST data...\n",
      "✓ Created 105,936 test windows\n",
      "  Each window shape: (60, 5) (60 timesteps × 5 features)\n",
      "\n",
      "✓ Saved X_train.npy\n",
      "✓ Saved X_test.npy\n",
      "\n",
      "============================================================\n",
      "WINDOW CREATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Ready for LSTM training!\n",
      "Training samples: 410,623\n",
      "Test samples: 105,936\n",
      "Window shape: (60 timesteps, 5 features)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: CREATING 60-SECOND WINDOWS (HR ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load normalized data\n",
    "train_df = pd.read_csv('dataset/train_data_normalized.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_normalized.csv')\n",
    "\n",
    "print(f\"\\nTrain rows: {len(train_df):,}\")\n",
    "print(f\"Test rows: {len(test_df):,}\")\n",
    "\n",
    "# Features to use in windows (only HR-based, no Speed!)\n",
    "feature_columns = [\n",
    "    'HR', \n",
    "    'HR_change', \n",
    "    'HR_rolling_mean', \n",
    "    'HR_rolling_std', \n",
    "    'HR_deviation'\n",
    "]\n",
    "\n",
    "print(f\"\\nUsing {len(feature_columns)} features per timestep\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "\n",
    "# Window size\n",
    "WINDOW_SIZE = 60  # 60 seconds\n",
    "\n",
    "def create_windows(df, window_size):\n",
    "    \"\"\"Create sliding windows from data\"\"\"\n",
    "    \n",
    "    windows = []\n",
    "    test_ids = []\n",
    "    \n",
    "    # Process each test separately\n",
    "    for test_id in df['ID_test'].unique():\n",
    "        test_data = df[df['ID_test'] == test_id][feature_columns].values\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for i in range(len(test_data) - window_size + 1):\n",
    "            window = test_data[i:i + window_size]\n",
    "            windows.append(window)\n",
    "            test_ids.append(test_id)\n",
    "    \n",
    "    return np.array(windows), test_ids\n",
    "\n",
    "# Create windows for training data\n",
    "print(\"\\nCreating windows for TRAIN data...\")\n",
    "X_train, train_test_ids = create_windows(train_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"✓ Created {len(X_train):,} training windows\")\n",
    "print(f\"  Each window shape: {X_train[0].shape} (60 timesteps × 5 features)\")\n",
    "\n",
    "# Create windows for test data\n",
    "print(\"\\nCreating windows for TEST data...\")\n",
    "X_test, test_test_ids = create_windows(test_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"✓ Created {len(X_test):,} test windows\")\n",
    "print(f\"  Each window shape: {X_test[0].shape} (60 timesteps × 5 features)\")\n",
    "\n",
    "# Save as numpy arrays\n",
    "np.save('dataset/X_train.npy', X_train)\n",
    "np.save('dataset/X_test.npy', X_test)\n",
    "\n",
    "print(\"\\n✓ Saved X_train.npy\")\n",
    "print(\"✓ Saved X_test.npy\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WINDOW CREATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nReady for LSTM training!\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Window shape: (60 timesteps, 5 features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d770b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
