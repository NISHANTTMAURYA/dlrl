{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f9e4ea",
   "metadata": {},
   "source": [
    "# üöÄ Updated Preprocessing Pipeline\n",
    "\n",
    "## ‚ú® What Changed?\n",
    "\n",
    "### ‚ùå **REMOVED** (Not needed for shock detection):\n",
    "1. **`time`** column\n",
    "   - LSTM learns temporal patterns from sequence order\n",
    "   - Absolute time is irrelevant for pattern recognition\n",
    "   \n",
    "2. **`Speed`** column  \n",
    "   - Not available on smartwatch\n",
    "   - Focus on HR patterns only\n",
    "   \n",
    "3. **`ID` and `ID_test`** from final data\n",
    "   - Kept during processing, removed before model training\n",
    "   - Prevents model from memorizing specific people\n",
    "\n",
    "### ‚úÖ **ADDED** (Better shock detection):\n",
    "1. **`HR_acceleration`** - 2nd derivative\n",
    "   - Exercise: Smooth, low acceleration\n",
    "   - Shock: Sharp, high acceleration\n",
    "   \n",
    "2. **`HR_change_abs`** - Magnitude of change\n",
    "   - Exercise: Small consistent values\n",
    "   - Shock: Large sudden values\n",
    "   \n",
    "3. **Data sampling (40%)**\n",
    "   - Keeps training fast\n",
    "   - Still representative of patterns\n",
    "\n",
    "### üìä **Final Features** (7 total):\n",
    "All normalized, ready for LSTM:\n",
    "1. `HR` - Current heart rate\n",
    "2. `HR_change` - First derivative\n",
    "3. `HR_acceleration` - Second derivative (NEW!)\n",
    "4. `HR_rolling_mean` - Recent average\n",
    "5. `HR_rolling_std` - Pattern variability\n",
    "6. `HR_deviation` - Distance from normal\n",
    "7. `HR_change_abs` - Change magnitude (NEW!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad8d70",
   "metadata": {},
   "source": [
    "# Treadmill Maximal Exercise Tests Dataset\n",
    "###### [Link](https://physionet.org/content/treadmill-exercise-cardioresp/1.0.1/)\n",
    "\n",
    "This dataset contains cardiorespiratory measurements taken during 992 treadmill maximal graded exercise tests conducted at the Exercise Physiology and Human Performance Lab, University of Malaga.\n",
    "\n",
    "## File: `test_measure.csv`\n",
    "\n",
    "This file contains all breath-by-breath cardiorespiratory measurements for each graded effort test.\n",
    "\n",
    "### General Info\n",
    "\n",
    "- **Rows:** 575,087 (one per breath measurement)\n",
    "- **Tests:** 992\n",
    "- **Median measurements per test:** 580 [IQR: 484‚Äì673]\n",
    "- **Median test duration:** 1,093.00 seconds [IQR: 978.75‚Äì1,208.00]\n",
    "\n",
    "### Variables\n",
    "\n",
    "| Name     | Description                                | Unit                  |\n",
    "|----------|--------------------------------------------|-----------------------|\n",
    "| time     | Time since measurement started             | seconds               |\n",
    "| Speed    | Treadmill speed                            | km/h                  |\n",
    "| HR       | Heart rate                                 | beats per minute      |\n",
    "| VO2      | Oxygen consumption                         | mL/min                |\n",
    "| VCO2     | Carbon dioxide production                  | mL/min                |\n",
    "| RR       | Respiration rate                           | respirations/min      |\n",
    "| VE       | Pulmonary ventilation                      | L/min                 |\n",
    "| ID       | Participant identification                 | -                     |\n",
    "| ID_test  | Effort test identification                 | -                     |\n",
    "\n",
    "_Note: VO2, VCO2, and VE are missing for 30 tests._\n",
    "\n",
    "**ID_test** is formatted as `{participant_id}_{test_number}`, e.g., `245_3` = third test of participant 245.\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:**  \n",
    "Mongin, D., Garc√≠a Romero, J., & Alvero Cruz, J. R. (2021). Treadmill Maximal Exercise Tests from the Exercise Physiology and Human Performance Lab of the University of Malaga (version 1.0.1). PhysioNet. https://doi.org/10.13026/7ezk-j442\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba9c72c-d382-45ff-b0b5-aaab4d63a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: REMOVING UNNECESSARY COLUMNS\n",
      "============================================================\n",
      "\n",
      "Original columns: ['time', 'Speed', 'HR', 'VO2', 'VCO2', 'RR', 'VE', 'ID_test', 'ID']\n",
      "Total rows: 575,087\n",
      "\n",
      "Columns after cleaning: ['HR', 'ID_test', 'ID']\n",
      "Removed columns: time, Speed, VO2, VCO2, RR, VE\n",
      "  ‚Ü≥ time: Not needed (LSTM learns temporal patterns from sequence order)\n",
      "  ‚Ü≥ Speed: Not available on smartwatch\n",
      "  ‚Ü≥ Others: Not relevant for HR spike detection\n",
      "\n",
      "First 10 rows of cleaned data:\n",
      "     HR ID_test  ID\n",
      "0  63.0     2_1   2\n",
      "1  75.0     2_1   2\n",
      "2  82.0     2_1   2\n",
      "3  87.0     2_1   2\n",
      "4  92.0     2_1   2\n",
      "5  94.0     2_1   2\n",
      "6  95.0     2_1   2\n",
      "7  96.0     2_1   2\n",
      "8  97.0     2_1   2\n",
      "9  97.0     2_1   2\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575087 entries, 0 to 575086\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   HR       574106 non-null  float64\n",
      " 1   ID_test  575087 non-null  object \n",
      " 2   ID       575087 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 13.2+ MB\n",
      "None\n",
      "\n",
      "============================================================\n",
      "‚úì STEP 1 COMPLETE!\n",
      "‚úì Saved as: dataset/output_step1.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#  clean the uncessary columns in the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('test_measure.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: REMOVING UNNECESSARY COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show what we have\n",
    "print(f\"\\nOriginal columns: {df.columns.tolist()}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "\n",
    "# Keep only the columns we need (ONLY HR and identifiers for processing)\n",
    "columns_to_keep = ['HR', 'ID_test', 'ID']\n",
    "\n",
    "df_cleaned = df[columns_to_keep]\n",
    "\n",
    "# Show what we kept\n",
    "print(f\"\\nColumns after cleaning: {df_cleaned.columns.tolist()}\")\n",
    "print(f\"Removed columns: time, Speed, VO2, VCO2, RR, VE\")\n",
    "print(\"  ‚Ü≥ time: Not needed (LSTM learns temporal patterns from sequence order)\")\n",
    "print(\"  ‚Ü≥ Speed: Not available on smartwatch\")\n",
    "print(\"  ‚Ü≥ Others: Not relevant for HR spike detection\")\n",
    "\n",
    "# Check the data\n",
    "print(\"\\nFirst 10 rows of cleaned data:\")\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\"\\nData info:\")\n",
    "print(df_cleaned.info())\n",
    "\n",
    "# Save to new CSV\n",
    "df_cleaned.to_csv('dataset/output_step1.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì STEP 1 COMPLETE!\")\n",
    "print(\"‚úì Saved as: dataset/output_step1.csv\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebd1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: SAMPLING & SPLITTING DATA\n",
      "============================================================\n",
      "\n",
      "Total participants: 857\n",
      "Sampled participants: 342 (40% for faster training)\n",
      "Rows after sampling: 231,886 (from 575,087)\n",
      "\n",
      "Training participants: 273\n",
      "Testing participants: 69\n",
      "\n",
      "Training rows: 189,459\n",
      "Testing rows: 42,427\n",
      "\n",
      "‚úì Saved dataset/train_data.csv\n",
      "‚úì Saved dataset/test_data.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# separate testing and training data + SAMPLE to reduce size\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned data from step 1\n",
    "df = pd.read_csv('dataset/output_step1.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: SAMPLING & SPLITTING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get unique participant IDs\n",
    "unique_participants = df['ID'].unique()\n",
    "print(f\"\\nTotal participants: {len(unique_participants)}\")\n",
    "\n",
    "# SAMPLE 40% of participants to keep dataset manageable\n",
    "np.random.seed(42)\n",
    "sampled_participants = np.random.choice(\n",
    "    unique_participants, \n",
    "    size=int(len(unique_participants) * 0.4),  # Keep 40% of data\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "print(f\"Sampled participants: {len(sampled_participants)} (40% for faster training)\")\n",
    "\n",
    "# Filter to sampled participants\n",
    "df_sampled = df[df['ID'].isin(sampled_participants)]\n",
    "print(f\"Rows after sampling: {len(df_sampled):,} (from {len(df):,})\")\n",
    "\n",
    "# Split sampled participants 80-20 for train/test\n",
    "train_ids, test_ids = train_test_split(\n",
    "    sampled_participants, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining participants: {len(train_ids)}\")\n",
    "print(f\"Testing participants: {len(test_ids)}\")\n",
    "\n",
    "# Split the data based on participant ID\n",
    "train_df = df_sampled[df_sampled['ID'].isin(train_ids)]\n",
    "test_df = df_sampled[df_sampled['ID'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nTraining rows: {len(train_df):,}\")\n",
    "print(f\"Testing rows: {len(test_df):,}\")\n",
    "\n",
    "# Save both files\n",
    "train_df.to_csv('dataset/train_data.csv', index=False)\n",
    "test_df.to_csv('dataset/test_data.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úì Saved dataset/train_data.csv\")\n",
    "print(\"‚úì Saved dataset/test_data.csv\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44513247",
   "metadata": {},
   "source": [
    "## üéØ Goal: One-Class Classification for Shock Detection\n",
    "\n",
    "**Problem:** Detect if a heart rate spike is due to **exercise** (normal) or **shock/panic** (abnormal)\n",
    "\n",
    "**Approach:** \n",
    "- Train LSTM Autoencoder on **exercise data only** (treadmill tests)\n",
    "- Model learns: \"This is what normal exercise-induced HR patterns look like\"\n",
    "- At deployment: Patterns that DON'T match ‚Üí Flag as anomaly (shock/panic)\n",
    "\n",
    "**Key Features (NO `time` needed!):**\n",
    "1. ‚úÖ `HR` - Current heart rate value\n",
    "2. ‚úÖ `HR_change` - Speed of change (1st derivative)\n",
    "3. ‚úÖ `HR_acceleration` - How change is changing (2nd derivative)\n",
    "4. ‚úÖ `HR_rolling_mean` - Recent average (context)\n",
    "5. ‚úÖ `HR_rolling_std` - Recent variability (smoothness)\n",
    "6. ‚úÖ `HR_deviation` - Distance from recent normal\n",
    "7. ‚úÖ `HR_change_abs` - Magnitude of changes\n",
    "\n",
    "**Why NO `time`?**\n",
    "- LSTM learns temporal patterns from **sequence order** automatically\n",
    "- What matters: Pattern of changes, not when they occur\n",
    "- Exercise at second 50 looks same as at second 500\n",
    "\n",
    "**Data Strategy:**\n",
    "- Sample 40% of participants ‚Üí Faster training, still representative\n",
    "- Split 80/20 for train/test ‚Üí Standard practice\n",
    "- 60-second windows ‚Üí Captures full pattern context\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68c5a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: CREATING FEATURES FOR SHOCK DETECTION\n",
      "============================================================\n",
      "\n",
      "Processing TRAIN data...\n",
      "‚úì Train rows: 189,459\n",
      "‚úì Features created: 9 columns\n",
      "\n",
      "Processing TEST data...\n",
      "‚úì Test rows: 42,427\n",
      "‚úì Features created: 9 columns\n",
      "\n",
      "All features:\n",
      "['HR', 'ID_test', 'ID', 'HR_change', 'HR_acceleration', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation', 'HR_change_abs']\n",
      "\n",
      "Features for MODEL (HR-based only):\n",
      "['HR', 'HR_change', 'HR_acceleration', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation', 'HR_change_abs']\n",
      "Total model features: 7\n",
      "\n",
      "Metadata columns (not for model):\n",
      "['ID_test', 'ID']\n",
      "\n",
      "Sample from train data:\n",
      "      HR  HR_change  HR_acceleration  HR_rolling_std  HR_deviation\n",
      "0  153.0        0.0              0.0        0.000000      0.000000\n",
      "1  157.0        4.0              0.0        2.828427      2.000000\n",
      "2  157.0        0.0             -4.0        2.309401      1.333333\n",
      "3  158.0        1.0              1.0        2.217356      1.750000\n",
      "4  158.0        0.0             -1.0        2.073644      1.400000\n",
      "5  158.0        0.0              0.0        1.940790      1.166667\n",
      "6  158.0        0.0              0.0        1.825742      1.000000\n",
      "7  159.0        1.0              1.0        1.832251      1.750000\n",
      "8  159.0        0.0             -1.0        1.810463      1.555556\n",
      "9  157.0       -2.0             -2.0        1.712698     -0.400000\n",
      "\n",
      "‚úì Saved dataset/train_data_with_features.csv\n",
      "‚úì Saved dataset/test_data_with_features.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# create features for shock vs exercise detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: CREATING FEATURES FOR SHOCK DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function to add features\n",
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Create features that distinguish exercise from shock/panic:\n",
    "    - Exercise: Gradual, smooth HR increase\n",
    "    - Shock/Panic: Sudden, erratic HR spike\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by ID_test to ensure correct temporal order\n",
    "    df = df.sort_values(['ID_test']).reset_index(drop=True)\n",
    "    \n",
    "    # Process each test separately\n",
    "    for test_id in df['ID_test'].unique():\n",
    "        mask = df['ID_test'] == test_id\n",
    "        \n",
    "        # 1. HR_change: First derivative (how much HR changed)\n",
    "        #    Exercise: Gradual positive changes\n",
    "        #    Shock: Large sudden jump\n",
    "        df.loc[mask, 'HR_change'] = df.loc[mask, 'HR'].diff()\n",
    "        \n",
    "        # 2. HR_acceleration: Second derivative (rate of change of change)\n",
    "        #    Exercise: Low acceleration (smooth)\n",
    "        #    Shock: High acceleration (sudden)\n",
    "        df.loc[mask, 'HR_acceleration'] = df.loc[mask, 'HR_change'].diff()\n",
    "        \n",
    "        # 3. HR_rolling_mean: Average HR over last 30 seconds\n",
    "        #    Provides context for current HR\n",
    "        df.loc[mask, 'HR_rolling_mean'] = df.loc[mask, 'HR'].rolling(\n",
    "            window=30, min_periods=1\n",
    "        ).mean()\n",
    "        \n",
    "        # 4. HR_rolling_std: Variability over last 30 seconds\n",
    "        #    Exercise: Low std (consistent)\n",
    "        #    Shock: High std (erratic)\n",
    "        df.loc[mask, 'HR_rolling_std'] = df.loc[mask, 'HR'].rolling(\n",
    "            window=30, min_periods=1\n",
    "        ).std()\n",
    "        \n",
    "        # 5. HR_deviation: How far is current HR from recent average\n",
    "        #    Shows if HR is unusually high/low\n",
    "        df.loc[mask, 'HR_deviation'] = df.loc[mask, 'HR'] - df.loc[mask, 'HR_rolling_mean']\n",
    "        \n",
    "        # 6. HR_change_abs: Magnitude of HR change (ignoring direction)\n",
    "        #    Exercise: Small values\n",
    "        #    Shock: Large values\n",
    "        df.loc[mask, 'HR_change_abs'] = df.loc[mask, 'HR_change'].abs()\n",
    "    \n",
    "    # Fill NaN values (from diff and rolling operations)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process training data\n",
    "print(\"\\nProcessing TRAIN data...\")\n",
    "train_df = pd.read_csv('dataset/train_data.csv')\n",
    "train_with_features = add_features(train_df)\n",
    "\n",
    "print(f\"‚úì Train rows: {len(train_with_features):,}\")\n",
    "print(f\"‚úì Features created: {train_with_features.shape[1]} columns\")\n",
    "\n",
    "# Process test data\n",
    "print(\"\\nProcessing TEST data...\")\n",
    "test_df = pd.read_csv('dataset/test_data.csv')\n",
    "test_with_features = add_features(test_df)\n",
    "\n",
    "print(f\"‚úì Test rows: {len(test_with_features):,}\")\n",
    "print(f\"‚úì Features created: {test_with_features.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nAll features:\")\n",
    "print(train_with_features.columns.tolist())\n",
    "\n",
    "print(\"\\nFeatures for MODEL (HR-based only):\")\n",
    "model_features = [\n",
    "    'HR',                # Current heart rate\n",
    "    'HR_change',         # Speed of change\n",
    "    'HR_acceleration',   # Acceleration (NEW!)\n",
    "    'HR_rolling_mean',   # Recent average\n",
    "    'HR_rolling_std',    # Recent variability\n",
    "    'HR_deviation',      # Distance from normal\n",
    "    'HR_change_abs'      # Magnitude of change (NEW!)\n",
    "]\n",
    "print(model_features)\n",
    "print(f\"Total model features: {len(model_features)}\")\n",
    "\n",
    "print(\"\\nMetadata columns (not for model):\")\n",
    "print(['ID_test', 'ID'])\n",
    "\n",
    "print(\"\\nSample from train data:\")\n",
    "print(train_with_features[['HR', 'HR_change', 'HR_acceleration', 'HR_rolling_std', 'HR_deviation']].head(10))\n",
    "\n",
    "# Save with features\n",
    "train_with_features.to_csv('dataset/train_data_with_features.csv', index=False)\n",
    "test_with_features.to_csv('dataset/test_data_with_features.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úì Saved dataset/train_data_with_features.csv\")\n",
    "print(\"‚úì Saved dataset/test_data_with_features.csv\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec4210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: NORMALIZING FEATURES\n",
      "============================================================\n",
      "\n",
      "Train rows: 189,459\n",
      "Test rows: 42,427\n",
      "\n",
      "Normalizing 7 features:\n",
      "  ‚úì HR\n",
      "  ‚úì HR_change\n",
      "  ‚úì HR_acceleration\n",
      "  ‚úì HR_rolling_mean\n",
      "  ‚úì HR_rolling_std\n",
      "  ‚úì HR_deviation\n",
      "  ‚úì HR_change_abs\n",
      "\n",
      "‚úì Scaler fitted on training data\n",
      "  Mean values learned: [ 1.47060799e+02 -1.12583725e-02 -5.23596134e-03]... (showing first 3)\n",
      "  Std values learned: [32.36364114  9.14558772 15.08045565]... (showing first 3)\n",
      "\n",
      "‚úì Both datasets normalized\n",
      "\n",
      "Sample normalized values (train data):\n",
      "         HR  HR_change  HR_acceleration  HR_deviation\n",
      "0  0.183515   0.001231         0.000347      0.005629\n",
      "1  0.307110   0.438600         0.000347      0.125427\n",
      "2  0.307110   0.001231        -0.264897      0.085494\n",
      "3  0.338009   0.110573         0.066658      0.110452\n",
      "4  0.338009   0.001231        -0.065964      0.089488\n",
      "5  0.338009   0.001231         0.000347      0.075511\n",
      "6  0.338009   0.001231         0.000347      0.065528\n",
      "7  0.368908   0.110573         0.066658      0.110452\n",
      "8  0.368908   0.001231        -0.065964      0.098805\n",
      "9  0.307110  -0.217454        -0.132275     -0.018330\n",
      "\n",
      "‚úì Saved dataset/train_data_normalized.csv\n",
      "‚úì Saved dataset/test_data_normalized.csv\n",
      "‚úì Saved dataset/scaler.pkl\n",
      "  ‚Ü≥ You'll need this to normalize smartwatch data in real-time!\n",
      "\n",
      "============================================================\n",
      "NORMALIZATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: NORMALIZING FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data with features\n",
    "train_df = pd.read_csv('dataset/train_data_with_features.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_with_features.csv')\n",
    "\n",
    "print(f\"\\nTrain rows: {len(train_df):,}\")\n",
    "print(f\"Test rows: {len(test_df):,}\")\n",
    "\n",
    "# ONLY normalize HR-based features (not ID columns!)\n",
    "features_to_normalize = [\n",
    "    'HR', \n",
    "    'HR_change',\n",
    "    'HR_acceleration',      # NEW!\n",
    "    'HR_rolling_mean', \n",
    "    'HR_rolling_std', \n",
    "    'HR_deviation',\n",
    "    'HR_change_abs'         # NEW!\n",
    "]\n",
    "\n",
    "print(f\"\\nNormalizing {len(features_to_normalize)} features:\")\n",
    "for feat in features_to_normalize:\n",
    "    print(f\"  ‚úì {feat}\")\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT the scaler ONLY on training data (prevent data leakage!)\n",
    "scaler.fit(train_df[features_to_normalize])\n",
    "\n",
    "print(\"\\n‚úì Scaler fitted on training data\")\n",
    "print(f\"  Mean values learned: {scaler.mean_[:3]}... (showing first 3)\")\n",
    "print(f\"  Std values learned: {scaler.scale_[:3]}... (showing first 3)\")\n",
    "\n",
    "# TRANSFORM both train and test using the SAME scaler\n",
    "train_df[features_to_normalize] = scaler.transform(train_df[features_to_normalize])\n",
    "test_df[features_to_normalize] = scaler.transform(test_df[features_to_normalize])\n",
    "\n",
    "print(\"\\n‚úì Both datasets normalized\")\n",
    "\n",
    "# Show normalized values\n",
    "print(\"\\nSample normalized values (train data):\")\n",
    "print(train_df[['HR', 'HR_change', 'HR_acceleration', 'HR_deviation']].head(10))\n",
    "\n",
    "# Save normalized data\n",
    "train_df.to_csv('dataset/train_data_normalized.csv', index=False)\n",
    "test_df.to_csv('dataset/test_data_normalized.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úì Saved dataset/train_data_normalized.csv\")\n",
    "print(\"‚úì Saved dataset/test_data_normalized.csv\")\n",
    "\n",
    "# Save the scaler (CRITICAL for deployment!)\n",
    "with open('dataset/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"‚úì Saved dataset/scaler.pkl\")\n",
    "print(\"  ‚Ü≥ You'll need this to normalize smartwatch data in real-time!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NORMALIZATION COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cfd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: REMOVING METADATA COLUMNS\n",
      "============================================================\n",
      "\n",
      "Original columns: ['HR', 'ID_test', 'ID', 'HR_change', 'HR_acceleration', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation', 'HR_change_abs']\n",
      "\n",
      "Removed: ['ID_test', 'ID']\n",
      "  ‚Ü≥ These are identifiers, not features\n",
      "  ‚Ü≥ Model should learn patterns, not memorize people\n",
      "\n",
      "Remaining columns (MODEL FEATURES): ['HR', 'HR_change', 'HR_acceleration', 'HR_rolling_mean', 'HR_rolling_std', 'HR_deviation', 'HR_change_abs']\n",
      "\n",
      "‚úì Updated dataset/train_data_normalized.csv\n",
      "‚úì Updated dataset/test_data_normalized.csv\n",
      "\n",
      "üìä Final features for LSTM model:\n",
      "  1. HR\n",
      "  2. HR_change\n",
      "  3. HR_acceleration\n",
      "  4. HR_rolling_mean\n",
      "  5. HR_rolling_std\n",
      "  6. HR_deviation\n",
      "  7. HR_change_abs\n",
      "\n",
      "Total features: 7\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove metadata columns (keep only model features)\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: REMOVING METADATA COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the normalized data\n",
    "train_df = pd.read_csv('dataset/train_data_normalized.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_normalized.csv')\n",
    "\n",
    "print(f\"\\nOriginal columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Remove ID columns (they're not features, just identifiers)\n",
    "columns_to_remove = ['ID_test', 'ID']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_remove)\n",
    "test_df = test_df.drop(columns=columns_to_remove)\n",
    "\n",
    "print(f\"\\nRemoved: {columns_to_remove}\")\n",
    "print(\"  ‚Ü≥ These are identifiers, not features\")\n",
    "print(\"  ‚Ü≥ Model should learn patterns, not memorize people\")\n",
    "\n",
    "print(f\"\\nRemaining columns (MODEL FEATURES): {train_df.columns.tolist()}\")\n",
    "\n",
    "# Save back to same files\n",
    "train_df.to_csv('dataset/train_data_normalized.csv', index=False)\n",
    "test_df.to_csv('dataset/test_data_normalized.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úì Updated dataset/train_data_normalized.csv\")\n",
    "print(f\"‚úì Updated dataset/test_data_normalized.csv\")\n",
    "\n",
    "print(\"\\nüìä Final features for LSTM model:\")\n",
    "feature_columns = train_df.columns.tolist()\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "    \n",
    "print(f\"\\nTotal features: {len(feature_columns)}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7056f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6: CREATING 60-SECOND WINDOWS\n",
      "============================================================\n",
      "\n",
      "Train rows: 189,459\n",
      "Test rows: 42,427\n",
      "\n",
      "Using ALL 7 features per timestep:\n",
      "  1. HR\n",
      "  2. HR_change\n",
      "  3. HR_acceleration\n",
      "  4. HR_rolling_mean\n",
      "  5. HR_rolling_std\n",
      "  6. HR_deviation\n",
      "  7. HR_change_abs\n",
      "\n",
      "Creating windows for TRAIN data...\n",
      "‚úì Created 189,400 training windows\n",
      "  Each window shape: (60, 7) (60 timesteps √ó 7 features)\n",
      "\n",
      "Creating windows for TEST data...\n",
      "‚úì Created 42,368 test windows\n",
      "  Each window shape: (60, 7) (60 timesteps √ó 7 features)\n",
      "\n",
      "‚úì Saved dataset/X_train.npy\n",
      "‚úì Saved dataset/X_test.npy\n",
      "\n",
      "============================================================\n",
      "üéâ PREPROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä Dataset Summary:\n",
      "  Training windows: 189,400\n",
      "  Test windows: 42,368\n",
      "  Window size: 60 timesteps\n",
      "  Features per timestep: 7\n",
      "\n",
      "‚úÖ Ready for LSTM training!\n",
      "‚úÖ Model will learn: Exercise patterns (normal)\n",
      "‚úÖ Model will detect: Shock/panic patterns (anomaly)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6: CREATING 60-SECOND WINDOWS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load normalized data\n",
    "train_df = pd.read_csv('dataset/train_data_normalized.csv')\n",
    "test_df = pd.read_csv('dataset/test_data_normalized.csv')\n",
    "\n",
    "print(f\"\\nTrain rows: {len(train_df):,}\")\n",
    "print(f\"Test rows: {len(test_df):,}\")\n",
    "\n",
    "# ALL columns are features now (no metadata)\n",
    "feature_columns = train_df.columns.tolist()\n",
    "\n",
    "print(f\"\\nUsing ALL {len(feature_columns)} features per timestep:\")\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Window size: 60 seconds\n",
    "WINDOW_SIZE = 60\n",
    "\n",
    "def create_windows(df, window_size):\n",
    "    \"\"\"\n",
    "    Create sliding 60-second windows from continuous data.\n",
    "    Each window = 60 timesteps √ó 7 features\n",
    "    \n",
    "    Note: We can't use ID_test anymore since we removed it.\n",
    "    We'll create windows by sliding through the entire dataset.\n",
    "    This assumes data is already sorted by test and time.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df.values  # Convert to numpy array\n",
    "    windows = []\n",
    "    \n",
    "    # Create sliding windows\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        windows.append(window)\n",
    "    \n",
    "    return np.array(windows)\n",
    "\n",
    "# Create windows for training data\n",
    "print(\"\\nCreating windows for TRAIN data...\")\n",
    "X_train = create_windows(train_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"‚úì Created {len(X_train):,} training windows\")\n",
    "print(f\"  Each window shape: {X_train[0].shape} ({WINDOW_SIZE} timesteps √ó {len(feature_columns)} features)\")\n",
    "\n",
    "# Create windows for test data\n",
    "print(\"\\nCreating windows for TEST data...\")\n",
    "X_test = create_windows(test_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"‚úì Created {len(X_test):,} test windows\")\n",
    "print(f\"  Each window shape: {X_test[0].shape} ({WINDOW_SIZE} timesteps √ó {len(feature_columns)} features)\")\n",
    "\n",
    "# Save as numpy arrays\n",
    "np.save('dataset/X_train.npy', X_train)\n",
    "np.save('dataset/X_test.npy', X_test)\n",
    "\n",
    "print(\"\\n‚úì Saved dataset/X_train.npy\")\n",
    "print(\"‚úì Saved dataset/X_test.npy\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  Training windows: {len(X_train):,}\")\n",
    "print(f\"  Test windows: {len(X_test):,}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE} timesteps\")\n",
    "print(f\"  Features per timestep: {len(feature_columns)}\")\n",
    "print(f\"\\n‚úÖ Ready for LSTM training!\")\n",
    "print(f\"‚úÖ Model will learn: Exercise patterns (normal)\")\n",
    "print(f\"‚úÖ Model will detect: Shock/panic patterns (anomaly)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d770b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
