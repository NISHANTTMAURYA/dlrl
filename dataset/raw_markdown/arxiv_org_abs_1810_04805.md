[Skip to main content](#content)

[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

In just 5 minutes help us improve arXiv:

[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)

We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)

[](/IgnoreMe)

[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) > [cs](/list/cs/recent) > arXiv:1810.04805 

[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)

All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text

Search

[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)

[ ![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)

open search

GO

open navigation menu

## quick links

  * [Login](https://arxiv.org/login)
  * [Help Pages](https://info.arxiv.org/help)
  * [About](https://info.arxiv.org/about)



# Computer Science > Computation and Language

**arXiv:1810.04805** (cs) 

[Submitted on 11 Oct 2018 ([v1](https://arxiv.org/abs/1810.04805v1)), last revised 24 May 2019 (this version, v2)]

# Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Authors:[Jacob Devlin](https://arxiv.org/search/cs?searchtype=author&query=Devlin,+J), [Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang,+M), [Kenton Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+K), [Kristina Toutanova](https://arxiv.org/search/cs?searchtype=author&query=Toutanova,+K)

View a PDF of the paper titled BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, by Jacob Devlin and 3 other authors

[View PDF](/pdf/1810.04805)

> Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   
> BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). 

Subjects: |  Computation and Language (cs.CL)  
---|---  
Cite as: | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) [cs.CL]  
  | (or  [arXiv:1810.04805v2](https://arxiv.org/abs/1810.04805v2) [cs.CL] for this version)   
  |  <https://doi.org/10.48550/arXiv.1810.04805> Focus to learn more arXiv-issued DOI via DataCite  
  
## Submission history

From: Ming-Wei Chang [[view email](/show-email/fbea950c/1810.04805)]   
**[[v1]](/abs/1810.04805v1)** Thu, 11 Oct 2018 00:50:01 UTC (227 KB)  
**[v2]** Fri, 24 May 2019 20:37:26 UTC (309 KB)  


Full-text links:

## Access Paper:

View a PDF of the paper titled BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, by Jacob Devlin and 3 other authors

  * [View PDF](/pdf/1810.04805)
  * [TeX Source ](/src/1810.04805)



[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ "Rights to this article")

Current browse context: 

cs.CL

[< prev](/prevnext?id=1810.04805&function=prev&context=cs.CL "previous in cs.CL \(accesskey p\)")   |   [next >](/prevnext?id=1810.04805&function=next&context=cs.CL "next in cs.CL \(accesskey n\)")   


[new](/list/cs.CL/new) |  [recent](/list/cs.CL/recent) | [2018-10](/list/cs.CL/2018-10)

Change to browse by: 

[cs](/abs/1810.04805?context=cs)  


### References & Citations

  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1810.04805)
  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1810.04805)
  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:1810.04805)



### [ 109 blog links](/tb/1810.04805)

([what is this?](https://info.arxiv.org/help/trackback.html)) 

### [DBLP](https://dblp.uni-trier.de) \- CS Bibliography

[listing](https://dblp.uni-trier.de/db/journals/corr/corr1810.html#abs-1810-04805 "listing on DBLP") | [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1810-04805 "DBLP bibtex record")

[Jacob Devlin](https://dblp.uni-trier.de/search/author?author=Jacob%20Devlin "DBLP author search")  
[Ming-Wei Chang](https://dblp.uni-trier.de/search/author?author=Ming-Wei%20Chang "DBLP author search")  
[Kenton Lee](https://dblp.uni-trier.de/search/author?author=Kenton%20Lee "DBLP author search")  
[Kristina Toutanova](https://dblp.uni-trier.de/search/author?author=Kristina%20Toutanova "DBLP author search")

export BibTeX citation Loading...

## BibTeX formatted citation

×

loading...

Data provided by: 

### Bookmark

[ ![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1810.04805&description=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "Bookmark on BibSonomy") [ ![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/1810.04805&title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "Bookmark on Reddit")

Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_

Connected Papers Toggle

Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_

Litmaps Toggle

Litmaps _([What is Litmaps?](https://www.litmaps.co/))_

scite.ai Toggle

scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_

Code, Data, Media

# Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_

Links to Code Toggle

CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_

DagsHub Toggle

DagsHub _([What is DagsHub?](https://dagshub.com/))_

GotitPub Toggle

Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_

Huggingface Toggle

Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_

Links to Code Toggle

Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_

ScienceCast Toggle

ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_

Demos

# Demos

Replicate Toggle

Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_

Spaces Toggle

Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_

Spaces Toggle

TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_

Core recommender toggle

CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_

  * Author
  * Venue
  * Institution
  * Topic



About arXivLabs 

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](/auth/show-endorsers/1810.04805) | [Disable MathJax](javascript:setMathjaxCookie\(\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 

  * [About](https://info.arxiv.org/about)
  * [Help](https://info.arxiv.org/help)



  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)
  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)



  * [Copyright](https://info.arxiv.org/help/license/index.html)
  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)



  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
  * [arXiv Operational Status ](https://status.arxiv.org)  




